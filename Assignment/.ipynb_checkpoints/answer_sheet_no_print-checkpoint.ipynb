{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from time import sleep\n",
    "import pickle\n",
    "\n",
    "rng = np.random\n",
    "\n",
    "class MCControl(object):\n",
    "    '''\n",
    "    Monte Carlo Control\n",
    "    '''\n",
    "    N_0 = 100.0\n",
    "    #N_0 = 1.0\n",
    "    #N_0 = 10.0\n",
    "    dict_sa_N = {}\n",
    "    dict_sa_Q = {}\n",
    "    dict_s_V ={}\n",
    "    game = None\n",
    "    num_game = 0.0\n",
    "    acc_reward =0.0\n",
    "    avg_reward =[]\n",
    "    \n",
    "    \n",
    "    class Easy21(object):\n",
    "        \"\"\" Simplified Blackjack Game Class\"\"\"\n",
    "        num_play = 0\n",
    "        num_step = 1\n",
    "        state_dealer = 0\n",
    "        state_player = 0\n",
    "        is_finish    = False\n",
    "        reward = 0\n",
    "\n",
    "        def draw_card(self):\n",
    "            number = rng.randint(10)+1\n",
    "            if rng.random() > (1.0/3.0):\n",
    "                rb = 1  #-1:red, 1:black\n",
    "            else:\n",
    "                rb = -1\n",
    "            return number,rb\n",
    "\n",
    "        def __init__(self):\n",
    "            self.num_play += 1\n",
    "            number, rb = self.draw_card()\n",
    "            self.state_dealer += number #only black\n",
    "            number, rb = self.draw_card()\n",
    "            self.state_player += number #only black\n",
    "\n",
    "        def print_state(self):\n",
    "            #print('step:' +str(self.num_step))\n",
    "            #print('dealer state:' + str(self.state_dealer))\n",
    "            #print('player state:' + str(self.state_player))\n",
    "            #print('reward:' + str(self.reward))\n",
    "            if self.is_finish:\n",
    "                #print('Finished!')\n",
    "                pass\n",
    "            else:\n",
    "                #print('On game.')\n",
    "                pass\n",
    "                \n",
    "        def get_state(self):\n",
    "            self.print_state()\n",
    "            return (self.state_dealer,self.state_player)\n",
    "\n",
    "        def step(self, state , action):\n",
    "            #self.print_states()\n",
    "            self.num_step += 1\n",
    "            if action == 0:\n",
    "                self.reward = self.finalize()\n",
    "                return (self.state_dealer, self.state_player), self.reward\n",
    "            elif action == 1:\n",
    "                number, rb = self.draw_card()\n",
    "                self.state_player += number * rb\n",
    "                if self.state_player > 21:\n",
    "                    self.reward = -1\n",
    "                    self.is_finish = True\n",
    "                if self.state_player < 1:\n",
    "                    self.reward = -1\n",
    "                    self.is_finish = True\n",
    "                return (self.state_dealer,self.state_player), self.reward\n",
    "        \n",
    "        def finalize(self):\n",
    "            self.is_finish = True\n",
    "            while self.state_dealer < 17:\n",
    "                number, rb = self.draw_card()\n",
    "                self.state_dealer += number * rb\n",
    "                if self.state_dealer < 1:\n",
    "                    return 1\n",
    "            if self.state_dealer > 21:\n",
    "                return 1\n",
    "            if self.state_dealer < self.state_player:\n",
    "                return 1\n",
    "            elif self.state_dealer == self.state_player:\n",
    "                return 0\n",
    "            elif self.state_dealer > self.state_player:\n",
    "                return -1\n",
    "        def interactive(self):\n",
    "            while self.is_finish == False:\n",
    "                state = self.get_state()\n",
    "                action = input(\"stick or hit(0/1):\")\n",
    "                state, reward = self.step(state, action)\n",
    "            self.get_state()\n",
    "\n",
    "            \n",
    "    def __init__(self):\n",
    "        rng.seed(123)\n",
    "        pass\n",
    "\n",
    "    def get_control(self,state):\n",
    "        #epsilon-greedy optimal control\n",
    "        epsilon = self.N_0 /(self.N_0 + self.getN(state,0)+ self.getN(state,1))\n",
    "        #epsilon = 0.1\n",
    "        if( np.random.random() > float(epsilon)):\n",
    "            #optimal policy\n",
    "            if(self.getQ(state,0) > self.getQ(state,1)):\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "        else:\n",
    "            #random policy\n",
    "            if(np.random.random() > 0.5):\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "\n",
    "    def simulate(self, num_iters):\n",
    "        for i in range(num_iters):\n",
    "            self.simulate_once()\n",
    "            if i % 100000 == 0:\n",
    "                #print('show')\n",
    "                self.showV()\n",
    "                self.saveQ()\n",
    "\n",
    "    def simulate_once(self):\n",
    "        self.num_game += 1\n",
    "        self.game = self.Easy21()\n",
    "        sas = []\n",
    "        state = self.game.get_state()                # initial state\n",
    "        action = self.get_control(state)             # control\n",
    "        sas.append((state,action))\n",
    "        state, reward = self.game.step(state, action) # environment gives the next state and reward\n",
    "        while self.game.is_finish == False:\n",
    "            action = self.get_control(state)         # agent choose the epsilon greedy control\n",
    "            sas.append((state,action))\n",
    "            state, reward = self.game.step(state,action) # environment gives the next state and reward\n",
    "        self.acc_reward += reward\n",
    "\n",
    "        # accumulate this episode in data structure N and Q\n",
    "        for i,(state,action) in enumerate(sas):\n",
    "            self.updateN(state,action)\n",
    "            self.updateQ(state,action,reward)\n",
    "        avg_reward = self.acc_reward /self.num_game\n",
    "        self.avg_reward.append(avg_reward)\n",
    "\n",
    "\n",
    "    def updateN(self,state,action):\n",
    "        key = (state,action)\n",
    "        if key in self.dict_sa_N:\n",
    "            self.dict_sa_N[key] += 1\n",
    "        else:\n",
    "            self.dict_sa_N[key] = 1\n",
    "\n",
    "    def getN(self,state,action):\n",
    "        key = (state,action)\n",
    "        if key in self.dict_sa_N:\n",
    "            return self.dict_sa_N[key]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def updateQ(self,state,action,reward):\n",
    "        alpha =1.0/float(self.getN(state,action))\n",
    "        key = (state,action)\n",
    "        if key in self.dict_sa_Q:\n",
    "            self.dict_sa_Q[key] = self.dict_sa_Q[key] + alpha * ( reward - self.dict_sa_Q[key])\n",
    "        else:\n",
    "            self.dict_sa_Q[key] = alpha * ( reward )\n",
    "\n",
    "    def getQ(self,state,action):\n",
    "        key = (state,action)\n",
    "        if key in self.dict_sa_Q:\n",
    "            return self.dict_sa_Q[key]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def saveQ(self):\n",
    "        pickle.dump( self.dict_sa_Q, open( \"MC_action_value_Q_func.p\", \"wb\" ) )\n",
    "\n",
    "    def buildV(self):\n",
    "        X = np.zeros((10,21),dtype=np.float)\n",
    "        Y = np.zeros((10,21),dtype=np.float)\n",
    "        Z = np.zeros((10,21),dtype=np.float)\n",
    "        for i in range(0,10):\n",
    "            for j in range(0,21):\n",
    "                state = (i+1,j+1) # index start from zero\n",
    "                self.dict_s_V[state] = np.max( (self.getQ(state,0),self.getQ(state,1)) ) #bellman optimal condition\n",
    "                X[i,j] = i+1\n",
    "                Y[i,j] = j+1\n",
    "                Z[i,j] = self.dict_s_V[state]\n",
    "        return X,Y,Z\n",
    "\n",
    "    def showV(self):\n",
    "        X,Y,Z = self.buildV()\n",
    "        fig = plt.figure(1)\n",
    "        fig.clf()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.plot_wireframe(X, Y, Z, rstride=1, cstride=1)\n",
    "        plt.xlim([1,10])\n",
    "        plt.xlabel('dealer showing', fontsize=18)\n",
    "        plt.ylim([1,21])\n",
    "        plt.ylabel('player sum', fontsize=18)\n",
    "        fig.suptitle('Monte Carlo Control', fontsize=20)\n",
    "        ax.set_xticks(range(1,11))\n",
    "        ax.set_yticks(range(1,22))\n",
    "\n",
    "        fig.savefig('mcc_answer.png')\n",
    "\n",
    "        #fig2 = plt.figure(2)\n",
    "        #fig2.clf()\n",
    "        #plt.plot(self.avg_reward)\n",
    "\n",
    "        #fig2.savefig('results/avg_reward.png')\n",
    "\n",
    "\n",
    "        plt.draw()\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.1)\n",
    "        plt.show()\n",
    "\n",
    "mcc = MCControl()\n",
    "#mcc.simulate_once()\n",
    "mcc.simulate(5000)\n",
    "mcc.showV()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc.showV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from time import sleep\n",
    "import pickle\n",
    "\n",
    "rng = np.random\n",
    "\n",
    "\n",
    "'''\n",
    "I have to rewrite this code to handle private variable later using variable mangling(?) using naming convention with prefix __.\n",
    "'''\n",
    "\n",
    "\n",
    "class SarsaControl:\n",
    "    '''\n",
    "    Monte Carlo Control\n",
    "    '''\n",
    "    N_0 = 100.0\n",
    "    #N_0 = 1.0\n",
    "    #N_0 = 10.0\n",
    "    dict_sa_N = {}\n",
    "    dict_sa_Q = {}\n",
    "\n",
    "    #add for Q function\n",
    "    dict_sa_Q_opt = {}\n",
    "    list_Q_diff = []\n",
    "\n",
    "    dict_sa_E = {} #eligibility trace\n",
    "    dict_s_V ={}\n",
    "    game = None\n",
    "    num_game = 0.0\n",
    "    acc_reward =0.0\n",
    "    avg_reward =[]\n",
    "    discount_rate = 1.0\n",
    "    __td_lambda = 0.0\n",
    "    \n",
    "    \n",
    "    class Easy21(object):\n",
    "        \"\"\" Simplified Blackjack Game Class\"\"\"\n",
    "        num_play = 0\n",
    "        num_step = 1\n",
    "        state_dealer = 0\n",
    "        state_player = 0\n",
    "        is_finish    = False\n",
    "        reward = 0\n",
    "\n",
    "        def draw_card(self):\n",
    "            number = rng.randint(10)+1\n",
    "            if rng.random() > (1.0/3.0):\n",
    "                rb = 1  #-1:red, 1:black\n",
    "            else:\n",
    "                rb = -1\n",
    "            return number,rb\n",
    "\n",
    "        def __init__(self):\n",
    "            self.num_play += 1\n",
    "            number, rb = self.draw_card()\n",
    "            self.state_dealer += number #only black\n",
    "            number, rb = self.draw_card()\n",
    "            self.state_player += number #only black\n",
    "\n",
    "        def print_state(self):\n",
    "            #print('step:' +str(self.num_step))\n",
    "            #print('dealer state:' + str(self.state_dealer))\n",
    "            #print('player state:' + str(self.state_player))\n",
    "            #print('reward:' + str(self.reward))\n",
    "            if self.is_finish:\n",
    "                #print('Finished!')\n",
    "                pass\n",
    "            else:\n",
    "                #print('On game.')\n",
    "                pass\n",
    "                \n",
    "        def get_state(self):\n",
    "            self.print_state()\n",
    "            return (self.state_dealer,self.state_player)\n",
    "\n",
    "        def step(self, state , action):\n",
    "            #self.print_states()\n",
    "            self.num_step += 1\n",
    "            if action == 0:\n",
    "                self.reward = self.finalize()\n",
    "                return (self.state_dealer, self.state_player), self.reward\n",
    "            elif action == 1:\n",
    "                number, rb = self.draw_card()\n",
    "                self.state_player += number * rb\n",
    "                if self.state_player > 21:\n",
    "                    self.reward = -1\n",
    "                    self.is_finish = True\n",
    "                if self.state_player < 1:\n",
    "                    self.reward = -1\n",
    "                    self.is_finish = True\n",
    "                return (self.state_dealer,self.state_player), self.reward\n",
    "        \n",
    "        def finalize(self):\n",
    "            self.is_finish = True\n",
    "            while self.state_dealer < 17:\n",
    "                number, rb = self.draw_card()\n",
    "                self.state_dealer += number * rb\n",
    "                if self.state_dealer < 1:\n",
    "                    return 1\n",
    "            if self.state_dealer > 21:\n",
    "                return 1\n",
    "            if self.state_dealer < self.state_player:\n",
    "                return 1\n",
    "            elif self.state_dealer == self.state_player:\n",
    "                return 0\n",
    "            elif self.state_dealer > self.state_player:\n",
    "                return -1\n",
    "        def interactive(self):\n",
    "            while self.is_finish == False:\n",
    "                state = self.get_state()\n",
    "                action = input(\"stick or hit(0/1):\")\n",
    "                state, reward = self.step(state, action)\n",
    "            self.get_state()\n",
    "\n",
    "    def __init__(self, td_lambda=0.5):\n",
    "        rng.seed(123)\n",
    "        self.__td_lambda = td_lambda\n",
    "        #self.dict_sa_Q_opt = pickle.load(open(\"results/MC_action_value_Q_func.p\",\"rb\"))\n",
    "        self.dict_sa_N = {}\n",
    "        self.dict_sa_Q = {}\n",
    "        self.list_Q_diff = []\n",
    "        self.avg_reward = []\n",
    "\n",
    "        pass\n",
    "\n",
    "    def get_control(self,state):\n",
    "        #epsilon-greedy optimal control\n",
    "        epsilon = self.N_0 /(self.N_0 + self.getN(state,0)+ self.getN(state,1))\n",
    "        #epsilon = 0.1\n",
    "        if( np.random.random() > float(epsilon)):\n",
    "            #optimal policy\n",
    "            if(self.getQ(state,0) > self.getQ(state,1)):\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "        else:\n",
    "            #random policy\n",
    "            if(np.random.random() > 0.5):\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "\n",
    "\n",
    "    def simulate(self, num_iters):\n",
    "        for i in range(num_iters):\n",
    "            self.simulate_once()\n",
    "            self.calQdiff()\n",
    "            if i % 10000 == 0:\n",
    "                #print('show')\n",
    "                self.showQdiff()\n",
    "                self.showV()\n",
    "\n",
    "    def simulate_once(self):\n",
    "        # game statistics\n",
    "        self.num_game += 1\n",
    "        # init eligibility trace\n",
    "        self.dict_sa_E = {}\n",
    "        # new game\n",
    "        self.game = self.Easy21()\n",
    "        state = self.game.get_state()\n",
    "        action = self.get_control(state)\n",
    "        #print('initial state and action : ', state, action)\n",
    "        while self.game.is_finish == False:\n",
    "            #print(self.game.is_finish)\n",
    "            self.updateN(state,action)\n",
    "            self.updateE(state,action)\n",
    "            next_state, reward = self.game.step(state,action) # environment\n",
    "            #print('next_state and reward : ', next_state, reward)\n",
    "            next_action = self.get_control(next_state)        # e-greedy policy\n",
    "            self.updateQ(state,action,reward,next_state,next_action)\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "        #debug\n",
    "        self.acc_reward += reward\n",
    "        avg_reward = self.acc_reward /self.num_game\n",
    "        self.avg_reward.append(avg_reward)\n",
    "        #print('# of games played:' +str(int(self.num_game))+' average reward:' + str(avg_reward))\n",
    "\n",
    "    def updateE(self,state,action):\n",
    "        key = (state,action)\n",
    "        if key in self.dict_sa_E:\n",
    "            self.dict_sa_E[key] += 1\n",
    "        else:\n",
    "            self.dict_sa_E[key] = 1\n",
    "    def getE(self,state,action):\n",
    "        key = (state,action)\n",
    "        if key in self.dict_sa_E:\n",
    "            return self.dict_sa_E[key]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def updateN(self,state,action):\n",
    "        key = (state,action)\n",
    "        if key in self.dict_sa_N:\n",
    "            self.dict_sa_N[key] += 1\n",
    "        else:\n",
    "            self.dict_sa_N[key] = 1\n",
    "\n",
    "    def getN(self,state,action):\n",
    "        key = (state,action)\n",
    "        if key in self.dict_sa_N:\n",
    "            return self.dict_sa_N[key]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    '''\n",
    "    def updateQ(self,state,action,reward,next_state,next_action):\n",
    "      alpha =1.0/float(self.getN(state,action))\n",
    "      key = (state,action)\n",
    "      if next_state == None:\n",
    "        if key in self.dict_sa_Q:\n",
    "            self.dict_sa_Q[key] = self.dict_sa_Q[key] + alpha * ( reward - self.dict_sa_Q[key])\n",
    "        else:\n",
    "            self.dict_sa_Q[key] = alpha * (reward )\n",
    "      else:\n",
    "        next_key = (next_state,next_action)\n",
    "        if next_key not in self.dict_sa_Q:\n",
    "            self.dict_sa_Q[next_key] = 0 # initialize next state if it doesn't exist\n",
    "        if key in self.dict_sa_Q:\n",
    "            self.dict_sa_Q[key] = self.dict_sa_Q[key] + alpha * ( reward +(self.discount_rate * self.dict_sa_Q[next_key])- self.dict_sa_Q[key])\n",
    "        else:\n",
    "            self.dict_sa_Q[key] = alpha * (reward )\n",
    "    '''\n",
    "\n",
    "    def updateQ(self,state,action,reward,next_state,next_action):\n",
    "        delta = reward + self.discount_rate * self.getQ(next_state,next_action) - self.getQ(state,action)\n",
    "        #print('next_state, next_action, Q_value and delta', next_state, next_action, self.getQ(next_state,next_action), delta)\n",
    "        for i in range(1,11):\n",
    "            for j in range(1,22):\n",
    "                for a in range(0,2):\n",
    "                    s = (i,j)\n",
    "                    alpha =1.0/float(1.0 + self.getN(s,a))\n",
    "                    #self.updateSpecificQ(s,a,alpha*delta)\n",
    "                    self.updateSpecificQ(s,a,alpha*delta*self.getE(s,a))\n",
    "                    self.updateSpecificE(s,a)\n",
    "\n",
    "\n",
    "    def updateSpecificQ(self,state,action,value):\n",
    "        key = (state,action)\n",
    "        if key in self.dict_sa_Q:\n",
    "            self.dict_sa_Q[key] += value\n",
    "        else:\n",
    "            self.dict_sa_Q[key] = value\n",
    "\n",
    "    def updateSpecificE(self,state,action):\n",
    "        key = (state, action)\n",
    "        if key in self.dict_sa_E:\n",
    "            self.dict_sa_E[key] = self.discount_rate * self.__td_lambda *self.dict_sa_E[key]\n",
    "        else:\n",
    "            self.dict_sa_E[key] = 0\n",
    "\n",
    "    def getQ(self,state,action):\n",
    "        key = (state,action)\n",
    "        if key in self.dict_sa_Q:\n",
    "            return self.dict_sa_Q[key]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def calQdiff(self):\n",
    "        diff = 0.0\n",
    "        for key in self.dict_sa_Q_opt.keys():\n",
    "            if key in self.dict_sa_Q:\n",
    "                diff += ( self.dict_sa_Q[key] - self.dict_sa_Q_opt[key])**2\n",
    "            else:\n",
    "                diff += ( self.dict_sa_Q_opt[key])**2\n",
    "        #print(\"Q difference from optimal Q: %f\"  % (diff))\n",
    "        self.list_Q_diff.append(diff)\n",
    "\n",
    "    def showQdiff(self):\n",
    "        fig3 = plt.figure(3)\n",
    "        fig3.clf()\n",
    "        plt.plot(self.list_Q_diff)\n",
    "\n",
    "        #fig3.savefig('results/list_Q_diff.png')\n",
    "\n",
    "\n",
    "    def buildV(self):\n",
    "        X = np.zeros((10,21),dtype=np.float)\n",
    "        Y = np.zeros((10,21),dtype=np.float)\n",
    "        Z = np.zeros((10,21),dtype=np.float)\n",
    "        for i in range(0,10):\n",
    "            for j in range(0,21):\n",
    "                state = (i+1,j+1) # index start from zero\n",
    "                self.dict_s_V[state] = np.max( (self.getQ(state,0),self.getQ(state,1)) )\n",
    "                X[i,j] = i+1\n",
    "                Y[i,j] = j+1\n",
    "                Z[i,j] = self.dict_s_V[state]\n",
    "        return X,Y,Z\n",
    "\n",
    "    def showV(self):\n",
    "        X,Y,Z = self.buildV()\n",
    "        fig = plt.figure(1)\n",
    "        fig.clf()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.plot_wireframe(X, Y, Z, rstride=1, cstride=1)\n",
    "        plt.xlim([1,10])\n",
    "        plt.xlabel('dealer showing', fontsize=18)\n",
    "        plt.ylim([1,21])\n",
    "        plt.ylabel('player sum', fontsize=18)\n",
    "        fig.suptitle('Sarsa(\\lambda) Control', fontsize=20)\n",
    "        ax.set_xticks(range(1,11))\n",
    "        ax.set_yticks(range(1,22))\n",
    "\n",
    "        #fig.savefig('results/srlc.png')\n",
    "\n",
    "        fig2 = plt.figure(2)\n",
    "        fig2.clf()\n",
    "        plt.plot(self.avg_reward)\n",
    "\n",
    "        #fig2.savefig('results/srlc_avg_reward.png')\n",
    "\n",
    "\n",
    "        plt.draw()\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.1)\n",
    "        #plt.show()\n",
    "\n",
    "############## Experiment 1 ###########################\n",
    "# 0.0 to 1.0\n",
    "list_lambda_Q = []\n",
    "for i in range(11):\n",
    "    td_lambda = float(i)/10.0\n",
    "    sc = SarsaControl(td_lambda)\n",
    "    sc.simulate(10000)\n",
    "    list_lambda_Q.append( sc.list_Q_diff[-1])\n",
    "    del sc\n",
    "\n",
    "fig4 = plt.figure(4)\n",
    "plt.plot(list_lambda_Q)\n",
    "plt.xticks([0,1,2,3,4,5,6,7,8,9.10], [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0])\n",
    "plt.draw()\n",
    "#fig4.savefig('results/q_diff_per_lambda.png')\n",
    "\n",
    "############## Experiment 2 ##########################\n",
    "# 0.0 vs 1.0\n",
    "sc = SarsaControl(td_lambda=1.0)\n",
    "sc.simulate(10000)\n",
    "list_Q_diff_one =  sc.list_Q_diff\n",
    "del sc\n",
    "\n",
    "sc = SarsaControl(td_lambda=0.0)\n",
    "sc.simulate(10000)\n",
    "list_Q_diff_zero =  sc.list_Q_diff\n",
    "del sc\n",
    "\n",
    "fig5 = plt.figure(5)\n",
    "plt.plot(list_Q_diff_zero,label= 'zero')\n",
    "plt.plot(list_Q_diff_one, label = 'one')\n",
    "plt.legend()\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('sum of square error')\n",
    "plt.title('comparison between zero vs one ')\n",
    "\n",
    "plt.draw()\n",
    "\n",
    "#fig5.savefig('results/list_Q_diff_zero_vs_one.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
