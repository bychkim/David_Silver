{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Game을 실행하는 start 함수 : 사용자가 직접 게임을 한다면, 초기 카드와 포지션에 대한 입력, 각 판당 결과와 승률 출력\n",
    "각 입력 시 입력 받은 값의 노출과 게임 진행 상황(total)들은 이 함수 안에서 출력 (실제 강화 학습에서는 각 게임당 결과나 진행 사항을 출력할 필요없음)\n",
    "\n",
    "Game을 진행하는 play 함수 : 실행 시에나 학습 시 직접 게임이 돌아가는 건 play로 구현. 나중에 강화 학습에서 agent가 될 부분. start를 실행하여 입력 받은 포지션에 따라서 play 함수를 호출하여 hit을 입력 받을 시에 사용자의 게임을 진행하고, stick을 입력 받을 시에 딜러의 게임을 자동으로 진행하여 게임 결과를 도출\n",
    "\n",
    "* play를 시작하면 player와 dealer의 초기 카드 초기화\n",
    "\n",
    "강화 학습 사용 시 class를 생성하여 학습 횟수를 입력 받을 때, 그 횟수만큼 학습을 진행하고 강화된 전략을 반영할 수 있어야 함\n",
    "user가 사용할 때랑 학습을 할 때의 전략을 구분?? \n",
    "\n",
    "Game을 play할 때 card를 draw하는 함수, 각 진행 사항마다 total 값 출력, 한 판이 끝날 때 마다 게임 결과와 승무패를 반영하여 저장,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Implementation of Easy21\n",
    "\n",
    "step 함수 : state(dealer's first card 1-10 and the player's sum 1-21) s와 action(hit or stick) a을 입력 받고 다음 state s'와 reward를 return (returns a sample of the next state s' which may be terminal if the game is finished) \n",
    "; 이 함수를 model-free RL의 environment로 사용할 것임. (there's no transition matrix for MDP)\n",
    "\n",
    "* dealer의 전략은 환경의 일부로 다루어야 함. 즉, stick에 대한 action을 입력 받게 되면, dealer가 게임을 시작하고 final reward와 terminal state를 return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(object):\n",
    "    def __init__(self):\n",
    "        self.count = {'state': 0, 'hit': 0, 'stick': 0}\n",
    "        self.action_value = {'hit':0, 'stick':0} # state value는 사용하지 않아도 되나? 사용할 경우 self.state_value 추가\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "state를 class로 관리하는데, 기본적으로 state는 tuple로 (dealer_card, player_total)의 형태로 구분한다.\n",
    "\n",
    "state 안에 또 class를 두어 각 state의 visit 횟수, 각 state에서 action(hit or stick)을 선택한 횟수, state value, action value를 관리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action):\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        state (tuple) : represent dealer_card and player_total in form of tuple (dealer_card, player_total)\n",
    "                       state - dealer's first card 1-10 and the player's sum 1-21\n",
    "        acntion (str) : 'hit' or 'stick'\n",
    "    \n",
    "    returns:\n",
    "        state (tuple) : next state determined by input action. It could be terminal\n",
    "        reward (int) : player's win +1, tie 0, lose -1\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if action == 'hit':\n",
    "        card = _draw_card()\n",
    "        \n",
    "        # update player's total \n",
    "        if card[1] == 'BLACK':\n",
    "            next_state = (state[0], state[1] + card[0])\n",
    "        elif card[1] == 'RED':\n",
    "            next_state = (state[0], state[1] - card[0])\n",
    "        \n",
    "        # calculate reward\n",
    "        if next_state[1] > 21 or next_state[1] < 1:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 0\n",
    "            \n",
    "    elif action == 'stick':\n",
    "        # dealer's play\n",
    "        while True:\n",
    "            card = _draw_card()\n",
    "            if card[1] == 'BLACK':\n",
    "                next_state = (state[0] + card[0], state[1])\n",
    "            elif card[1] == 'RED':\n",
    "                next_state = (state[0] - card[0], state[1])\n",
    "            \n",
    "            if next_state[0] >= 17 or next_state[0] < 1:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # calculate reward \n",
    "        if next_state[0] > 21 or next_state[0] < 1:\n",
    "            reward = 1\n",
    "        elif next_state[0] < next_state[1]:\n",
    "            reward = 1\n",
    "        elif next_state[0] > next_state[1]:\n",
    "            reward = -1\n",
    "        elif next_state[0] == next_state[0]:\n",
    "            reward = 0\n",
    "\n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _draw_card():\n",
    "    rand_number = random.randrange(1,11)\n",
    "    color_number = random.randrange(1,4)\n",
    "    \n",
    "    if color_number == 1:\n",
    "        color = 'RED'\n",
    "    else:\n",
    "        color = 'BLACK'\n",
    "    \n",
    "    return (rand_number, color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state):\n",
    "    # N_0 is constant. Feel free to choose an alternative value for N0, if it helps producing better results.\n",
    "    N_constant = 100\n",
    "    \n",
    "    random_action = random.random()\n",
    "    epsilon = N_constant / (N_constant + state_dict[state].count['state'])\n",
    "    if state_dict[state].action_value['hit'] > state_dict[state].action_value['stick']:\n",
    "        hit_prob = (epsilon / 2) + 1 - epsilon\n",
    "        #stick_prob = 1 - hit_prob\n",
    "    else:\n",
    "        hit_prob = (epsilon / 2)\n",
    "    \n",
    "    if random_action < hit_prob:\n",
    "        action = 'hit'\n",
    "    else:\n",
    "        action = 'stick'\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {} # initialization 초기화\n",
    "\n",
    "for dealer_card in range(1, 11):\n",
    "    for player_total in range(1, 22):\n",
    "        state_dict[(dealer_card, player_total)] = State()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_learning = 1000 # 학습 횟수\n",
    " \n",
    "for num in range(0, number_of_learning):\n",
    "    initial_state = (random.randrange(1,11), random.randrange(1,11))\n",
    "    initial_action = epsilon_greedy_policy(initial_state)\n",
    "    cur_state = initial_state\n",
    "    cur_action = initial_action\n",
    "    state_dict[cur_state].count['state'] += 1\n",
    "    state_dict[cur_state].count[cur_action] += 1\n",
    "    \n",
    "    # terminal state까지 반복\n",
    "    while True:  \n",
    "        (next_state, reward) = step(cur_state, cur_action)\n",
    "        if cur_action == 'stick' or reward == -1 : # terminal state\n",
    "            break\n",
    "        else:\n",
    "            cur_state = next_state\n",
    "            cur_action = epsilon_greedy_policy(next_state)\n",
    "            state_dict[cur_state].count['state'] += 1\n",
    "            state_dict[cur_state].count[cur_action] += 1\n",
    "            continue\n",
    "    \n",
    "    # Q value update\n",
    "    \"\"\"Monte-Carlo Control\n",
    "    N(S_t, A_t) = N(S_t, A_t) + 1 # 위에서 처리\n",
    "    Q(S_t, A_t) = Q(S_t, A_t) + 1/N(S_t, A_t) * (G_t - Q(S_t, A_t) )\n",
    "    \"\"\"\n",
    "    #temp_state = cur_state\n",
    "    #temp_action = cur_action \n",
    "    state_dict[cur_state].action_value[cur_action] = state_dict[cur_state].action_value[cur_action] + 1 / state_dict[cur_state].count[cur_action] * ( reward - state_dict[cur_state].action_value[cur_action] )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# state-action pair의 visit count를 update할 때 고려해야할 사항\n",
    "\n",
    "- terminal state에 도달하는 경우\n",
    "    - 즉, stick을 했거나, hit을 해서 bust가 일어난 경우에는 기존에 다루지 않는 state들이 나온다. (예를 들어 (-9, 1)과 같이)\n",
    "    - 이 경우 visit count를 update할 필요가 없다. \n",
    "\n",
    "- reward(or return G)를 계산하는 방법\n",
    "    - bust가 일어나지 않고, stick을 하지 않는 경우 (stick을 하면 무조건 reward가 나온다. 1이든 0이든 -1이든) \n",
    "    - terminal에 도달하기 전에 intermediate state에서는 reward가 항상 0이므로, \n",
    "    - 하나의 episode를 무조건 terminal까지 뽑아서 reward가 나오면 Q value를 update한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'hit': 2, 'state': 8, 'stick': 6}, {'hit': -1.0, 'stick': 1.0})"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict[(1, 21)].count, state_dict[(1,21)].action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'hit': 5, 'state': 10, 'stick': 5}, {'hit': -1.0, 'stick': 1.0})"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict[(2, 21)].count, state_dict[(2, 21)].action_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'hit': 2, 'state': 8, 'stick': 6}, {'hit': -1.0, 'stick': 1.0})"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict[(3, 21)].count, state_dict[(3, 21)].action_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'hit': 3, 'state': 5, 'stick': 2},\n",
       " {'hit': -0.3333333333333333, 'stick': 1.0})"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict[(4, 21)].count, state_dict[(4, 21)].action_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'hit': 2, 'state': 2, 'stick': 0}, {'hit': -1.0, 'stick': 0})"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict[(5, 21)].count, state_dict[(5, 21)].action_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'hit': 36, 'state': 131, 'stick': 95}, {'hit': 0, 'stick': 1.0})"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict[(5, 10)].count, state_dict[(5, 10)].action_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'hit': 37, 'state': 133, 'stick': 96},\n",
       " {'hit': -0.1954022988505747, 'stick': 1.0})"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict[(5, 5)].count, state_dict[(5, 5)].action_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'hit': 38, 'state': 143, 'stick': 105},\n",
       " {'hit': -0.037037037037037035, 'stick': 1.0})"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict[(1, 10)].count, state_dict[(1, 10)].action_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'hit': 114, 'state': 157, 'stick': 43},\n",
       " {'hit': -0.6051509413516918, 'stick': -0.9069767441860467})"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict[(10, 1)].count, state_dict[(10, 1)].action_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'hit': 80, 'state': 137, 'stick': 57},\n",
       " {'hit': -0.7251913425486668, 'stick': -0.5789473684210527})"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict[(9, 1)].count, state_dict[(9, 1)].action_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'hit': 48, 'state': 132, 'stick': 84},\n",
       " {'hit': -1.0, 'stick': -0.11904761904761905})"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict[(8, 1)].count, state_dict[(8, 1)].action_value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "처음 입력 받은 횟수만큼 학습을 한 후, 학습된 모델을 저장하여 이후에 추가 학습을 할 수 있도록 구현하는 것이 맞다.\n",
    "\n",
    "학습을 새로 시작할 때마다 모델이 초기화 되면 안된다.\n",
    "\n",
    "그러기 위해서는 agent 별로 학습을 할 수 있도록 해야한다. (변수 저장과 type의 문제)\n",
    "\n",
    "업데이트된 Q value는 함수형태로 저장하면 return과 동시에 초기화 되므로 적합하지 않다. (맞는가?)\n",
    "\n",
    "매 step또는 episode 별로 저장해서 업데이트 할 수 있어야 한다.\n",
    "\n",
    "policy를 저장하는 방법은??\n",
    "\n",
    "사실상 Q가 policy에 의해 결정되므로, 포함한다고 봐도 되나?\n",
    "\n",
    "반대로 policy도 epsilon-greedy 경우 Q만 보고 결정하면 되므로 (stochastic으로 주어지니)\n",
    "\n",
    "deterministic하게 정해서 저장할 수가 없다. \n",
    "\n",
    "특히 이 문제는 epsilon도 t에 따라 변화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
